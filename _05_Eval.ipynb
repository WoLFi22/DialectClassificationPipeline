{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1497c118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import matplotlib.colors as mcolors\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pickle\n",
    "\n",
    "matplotlib.use(\"Agg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba2517d",
   "metadata": {},
   "source": [
    "## get Boxplot and raincloudplot of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed80de0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplot(column, name, name_aug):\n",
    "    df = pd.read_pickle('./Results_' + name + '_' + name_aug + '.pkl')\n",
    "    dat = df[column].tolist()\n",
    "    print(column)\n",
    "    print('std:', np.std(dat))\n",
    "    print('median:', np.median(dat))\n",
    "    print('mean:', np.mean(dat))\n",
    "    data = [dat]\n",
    "    fig = plt.figure(figsize =(10, 7))\n",
    "    ax = fig.add_axes([0, 0, 1, 1])\n",
    "    plt.title(\"Boxplot of \" + column)\n",
    "    bp = ax.boxplot(data, showmeans=True)\n",
    "    ax.set_ylim(0,0.5)\n",
    "    plt.savefig('boxplot_' + column + '_' + name + '_' + name_aug + '.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    raincloudplot(column, name, name_aug)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "980017de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raincloudplot(column, name, name_aug):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 2))\n",
    "\n",
    "    boxplot_colors = ['yellowgreen']\n",
    "    violin_colors = ['purple']\n",
    "    scatter_colors = ['tomato']\n",
    "\n",
    "    df = pd.read_pickle('./Results_' + name + '_' + name_aug + '.pkl')\n",
    "    data = [df[column].tolist()]\n",
    "\n",
    "    bp = ax.boxplot(data, patch_artist=True, vert=False)\n",
    "\n",
    "    for patch, color in zip(bp['boxes'], boxplot_colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.4)\n",
    "\n",
    "    vp = ax.violinplot(data, points=500, \n",
    "                       showmeans=False, showextrema=False, showmedians=False, vert=False)\n",
    "\n",
    "    for idx, b in enumerate(vp['bodies']):\n",
    "        m = np.mean(b.get_paths()[0].vertices[:, 0])\n",
    "        b.get_paths()[0].vertices[:, 1] = np.clip(b.get_paths()[0].vertices[:, 1], idx + 1, idx + 2)\n",
    "        b.set_color(violin_colors[idx])\n",
    "\n",
    "    for idx, features in enumerate(data):\n",
    "        y = np.full(len(features), idx + .8)\n",
    "        idxs = np.arange(len(y))\n",
    "        out = y.astype(float)\n",
    "        out.flat[idxs] += np.random.uniform(low=-.1, high=.1, size=len(idxs))\n",
    "        y = out\n",
    "        plt.scatter(features, y, s=.3, c=scatter_colors[idx])\n",
    "\n",
    "    plt.yticks(np.arange(1, 2, 1), [name])\n",
    "    plt.xlabel('Scores')\n",
    "    ax.set_xlim(0,0.5)\n",
    "    plt.title(\"RainCloudPlot of \" + column)\n",
    "    plt.savefig('raincloudplot_' + column + '_' + name + '_' + name_aug + '.png', bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a27c8a",
   "metadata": {},
   "source": [
    "## confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fe9fe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusionMatrix(name, name_aug, mapping, absolute):\n",
    "    df_results = pd.read_pickle('./Results_' + name + '_' + name_aug + '.pkl')\n",
    "    \n",
    "    y_true = np.concatenate(df_results['classes_true'].tolist()).ravel()\n",
    "    y_pred = np.concatenate(df_results['classes_x'].tolist()).ravel()\n",
    "\n",
    "    mapping = {key: np.argmax(value) for key, value in mapping.items()}\n",
    "    print(\"Label mapping: \", mapping)\n",
    "    reversed_mapping = {value: key for key, value in mapping.items()}\n",
    "    true_mapped_list = [reversed_mapping[key] for key in y_true]\n",
    "    pred_mapped_list = [reversed_mapping[key] for key in y_pred]\n",
    "    cm_labels = np.unique(true_mapped_list)\n",
    "                  \n",
    "    matrix = confusion_matrix(true_mapped_list, pred_mapped_list)\n",
    "\n",
    "    if absolute:\n",
    "        df_cfm = pd.DataFrame(matrix, index=cm_labels, columns=cm_labels)\n",
    "    else:\n",
    "        row_sums = matrix.sum(axis=1, keepdims=True)\n",
    "        normalized_matrix = matrix / row_sums\n",
    "        df_cfm = pd.DataFrame(normalized_matrix, index=cm_labels, columns=cm_labels)\n",
    "    \n",
    "    plt.figure(figsize = (10,7))\n",
    "    cfm_plot = sn.heatmap(df_cfm, annot=True, fmt='.2f' if not absolute else 'g')\n",
    "    \n",
    "    plt.xlabel('Predicted label', fontsize=12)\n",
    "    plt.ylabel('True label', fontsize=12)\n",
    "    cfm_plot.set_xticklabels(cfm_plot.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n",
    "    cfm_plot.set_yticklabels(cfm_plot.get_yticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if absolute:\n",
    "        if '_combined' in name_aug:\n",
    "            cfm_plot.figure.savefig(\"confusion_matrix_combined.png\")\n",
    "        else:\n",
    "            cfm_plot.figure.savefig(\"confusion_matrix.png\")\n",
    "    else:\n",
    "        if '_combined' in name_aug:\n",
    "            cfm_plot.figure.savefig(\"normalized_confusion_matrix_combined.png\")\n",
    "        else:\n",
    "            cfm_plot.figure.savefig(\"normalized_confusion_matrix.png\")\n",
    "    plt.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4282380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speakerFalse(name, name_aug):\n",
    "    df_results = pd.read_pickle('./Results_' + name + '_' + name_aug + '.pkl')\n",
    "    df_data = pd.read_pickle('./All_Files_.pkl')\n",
    "    dialects = df_data['dialect'].unique().tolist()\n",
    "    speaker = [[] for _ in range(0, len(dialects))]\n",
    "    \n",
    "    all_speaker = np.concatenate(df_results['y_test_speaker'].tolist()).ravel()\n",
    "    false_speaker = np.concatenate(df_results['false_speaker'].tolist()).ravel()\n",
    "    \n",
    "    all_speaker_test = []\n",
    "    \n",
    "    for index, value in df_results['speaker_test'].iteritems():\n",
    "        flat_array = np.array([item for sublist in value for item in sublist])\n",
    "        all_speaker_test.extend(flat_array)\n",
    "    \n",
    "    for i in range(0, len(dialects)):\n",
    "        speaker[i] = df_data[(df_data['dialect'] == dialects[i]) & (df_data['augmented'] == 'False')]['speaker'].unique().tolist()\n",
    "        perc = []\n",
    "        total = []\n",
    "        for j in range(0, len(speaker[i])):\n",
    "            cnt_all = all_speaker.tolist().count(speaker[i][j])\n",
    "            cnt_false = false_speaker.tolist().count(speaker[i][j])\n",
    "            try:\n",
    "                perc.append(round((100/cnt_all)*cnt_false, 1))\n",
    "            except ZeroDivisionError:\n",
    "                print('warning:', speaker[i][j], 'is 0 times in the train-set.')\n",
    "                perc.append(0)\n",
    "            cnt_speaker = all_speaker_test.count(speaker[i][j])\n",
    "            total.append(cnt_speaker)\n",
    "        \n",
    "        fig, (cax, sax, lax, ax) = plt.subplots(nrows=4, figsize=(10,5),  gridspec_kw={\"height_ratios\":[0.3, 0.2, 0.3, 0.7]})\n",
    "        cfm_plot = sn.heatmap([perc], annot=True, square=True, vmin=0, vmax=100, xticklabels=speaker[i],\n",
    "                              yticklabels=False, ax=ax, cbar=False)\n",
    "        cfm_plot.axis('tight')\n",
    "        colors = ['black', 'white']\n",
    "        cmap = mcolors.ListedColormap(colors)\n",
    "        cfm_plot2 = sn.heatmap([total], annot=True, square=True, cmap=cmap, vmin=0, vmax=10, xticklabels=False,\n",
    "                               yticklabels=False, ax=lax, cbar=False, linewidths = 2, linecolor = \"black\")\n",
    "        cfm_plot3 = sn.heatmap([total], xticklabels=False, yticklabels=False, ax=sax, cbar=False, cmap=cmap, vmin=-5, vmax=-1)\n",
    "        cfm_plot2.axis('tight')\n",
    "        fig.colorbar(ax.get_children()[0], cax=cax, orientation=\"horizontal\")\n",
    "        for _, spine in cfm_plot2.spines.items():\n",
    "            spine.set_visible(True)\n",
    "        \n",
    "        fig.tight_layout()\n",
    "        cfm_plot.figure.savefig('confusion_matrix_' + dialects[i] + '.png')\n",
    "        plt.close(fig)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6000d7",
   "metadata": {},
   "source": [
    "## calcualte weighted f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5447c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_f1(name, name_aug, mapping):\n",
    "    df_results = pd.read_pickle('./Results_' + name + '_' + name_aug + '.pkl')\n",
    "    if (name_aug != ''):\n",
    "        df_data = pd.read_pickle('./Data_' + name_aug + '_aug.pkl')\n",
    "    else:\n",
    "        df_data = pd.read_pickle('./Data_.pkl')\n",
    "    \n",
    "    mapping = {key: np.argmax(value) for key, value in mapping.items()}\n",
    "    \n",
    "    class_weights = df_data['dialect'].value_counts(normalize=True).to_dict()\n",
    "    class_weights = {name: weight for name, weight in class_weights.items()}\n",
    "    flattened_list = [item for sublist in df_results['classes_true'] for item in sublist]\n",
    "    counts = Counter(flattened_list)\n",
    "    \n",
    "    y_true = np.concatenate(df_results['classes_true'].tolist()).ravel()\n",
    "    y_pred = np.concatenate(df_results['classes_x'].tolist()).ravel()\n",
    "    \n",
    "    TP_list = []\n",
    "    FP_list = []\n",
    "    TN_list = []\n",
    "    FN_list = []\n",
    "    F1_list = []\n",
    "    weights_list_all = []\n",
    "    weights_list_test = []\n",
    "    \n",
    "    for label, val in class_weights.items():\n",
    "        mapped_label = mapping[label]\n",
    "\n",
    "        cm = confusion_matrix(y_true == mapped_label, y_pred == mapped_label)\n",
    "    \n",
    "        TP = cm[1, 1]\n",
    "        FP = cm[0, 1]\n",
    "        TN = cm[0, 0]\n",
    "        FN = cm[1, 0]\n",
    "    \n",
    "        TP_list.append(TP)\n",
    "        FP_list.append(FP)\n",
    "        TN_list.append(TN)\n",
    "        FN_list.append(FN)\n",
    "        \n",
    "        F1_list.append(TP/(TP+0.5*(FP+FN)))\n",
    "        weights_list_all.append(val)\n",
    "        \n",
    "        weights_list_test.append(counts[mapped_label])\n",
    "\n",
    "    weighted_F1_score_all = np.sum([w * f1 for w, f1 in zip(weights_list_all, F1_list)]) / np.sum(weights_list_all)\n",
    "    print('weighted over All F1 score:', weighted_F1_score_all)\n",
    "    \n",
    "    weighted_F1_score_test = np.sum([w * f1 for w, f1 in zip(weights_list_test, F1_list)]) / np.sum(weights_list_test)\n",
    "    print('weighted over Test F1 score:', weighted_F1_score_test)\n",
    "    \n",
    "    names = list(class_weights.keys())\n",
    "    sorted_names = sorted(names)\n",
    "    sorted_f1_scores = [F1_list[names.index(name)] for name in sorted_names]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    y_pos = np.arange(len(sorted_names))\n",
    "    plt.barh(y_pos, sorted_f1_scores, color='blue')\n",
    "    plt.yticks(y_pos, sorted_names)\n",
    "    plt.xlabel('F1-Score')\n",
    "    plt.title('F1-Scores per class')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.savefig('f1_per_class.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4100324a",
   "metadata": {},
   "source": [
    "## Scores of combined Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f7b1de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_scores(name, name_aug):\n",
    "    data = pd.read_pickle('./Results_' + name + '_' + name_aug + '.pkl')\n",
    "\n",
    "    classes_x, classes_true, y_test_speaker = data['classes_x'], data['classes_true'], data['y_test_speaker']\n",
    "\n",
    "    f1_scores = []\n",
    "    conf_matrices = []\n",
    "    accuracies = []\n",
    "    all_reduced_data = []\n",
    "\n",
    "    df_new = pd.DataFrame(columns=['classes_x', 'classes_true', 'y_test_speaker'])\n",
    "\n",
    "    for row_classes_x, row_classes_true, row_speaker in zip(classes_x, classes_true, y_test_speaker):\n",
    "        speaker_predictions = {}\n",
    "        speaker_true = {}\n",
    "    \n",
    "        unique_speakers = set(row_speaker)\n",
    "        for speaker in unique_speakers:\n",
    "            speaker_indices = [i for i, s in enumerate(row_speaker) if s == speaker]\n",
    "            speaker_classes_x = row_classes_x[speaker_indices]\n",
    "            speaker_classes_true = row_classes_true[speaker_indices]\n",
    "            most_predicted_class = np.argmax(np.bincount(speaker_classes_x))\n",
    "            speaker_predictions[speaker] = most_predicted_class\n",
    "            speaker_true[speaker] = speaker_classes_true[0]\n",
    "\n",
    "        true_labels = np.array(list(speaker_true.values()))\n",
    "        predicted_labels = np.array(list(speaker_predictions.values()))\n",
    "\n",
    "        f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        correct_count = np.sum(predicted_labels == true_labels)\n",
    "        accuracy = correct_count / len(true_labels)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "        df_new.loc[len(df_new)] = [predicted_labels, true_labels, list(unique_speakers)]\n",
    "\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    median_f1 = np.median(f1_scores)\n",
    "    std_f1 = np.std(f1_scores)\n",
    "\n",
    "    print(\"Mean F1 Score:\", mean_f1)\n",
    "    print(\"Median F1 Score:\", median_f1)\n",
    "    print(\"Std F1 Score:\", std_f1)\n",
    "\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    median_accuracy = np.median(accuracies)\n",
    "    std_accuracy = np.std(accuracies)\n",
    "\n",
    "    print(\"Mean Accuracy:\", mean_accuracy)\n",
    "    print(\"Median Accuracy:\", median_accuracy)\n",
    "    print(\"Std Accuracy:\", std_accuracy)\n",
    "\n",
    "    df_new.to_pickle('./Results_' + name + '_' + name_aug + '_combined' + '.pkl')\n",
    "    df_new.to_csv('./Results_' + name + '_' + name_aug + '_combined' + '.csv',  sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32afb44a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
