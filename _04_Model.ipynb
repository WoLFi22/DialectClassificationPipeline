{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e1a8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Activation, LeakyReLU, Conv1D, GlobalAveragePooling1D, Flatten, MaxPooling1D,  BatchNormalization \n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from keras.utils import to_categorical\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "from random import sample\n",
    "import tensorflow as tf\n",
    "import keras.metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a984caee",
   "metadata": {},
   "source": [
    "## GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db49357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b6aec3",
   "metadata": {},
   "source": [
    "## divide intro train and test\n",
    "### Parameters:\n",
    "\n",
    "- **speaker_test_val**:  \n",
    "  A list of dialects with each sublist containing a list of speakers. The first half of each sublist is designated for testing, and the second half is for validation.\n",
    "\n",
    "- **df**:  \n",
    "  The original DataFrame containing audio data.\n",
    "\n",
    "- **df_aug**:  \n",
    "  The augmented DataFrame, if available.\n",
    "\n",
    "- **name_aug**:  \n",
    "  A name identifier used for loading augmented data or specifying augmentation settings.\n",
    "\n",
    "### Returns:\n",
    "\n",
    "- **x_train**:  \n",
    "  Numpy array of features for the training set.\n",
    "\n",
    "- **y_train**:  \n",
    "  List of dialect labels for the training set.\n",
    "\n",
    "- **x_test**:  \n",
    "  Numpy array of features for the testing set.\n",
    "\n",
    "- **y_test**:  \n",
    "  List of dialect labels for the testing set.\n",
    "\n",
    "- **x_val**:  \n",
    "  Numpy array of features for the validation set.\n",
    "\n",
    "- **y_val**:  \n",
    "  List of dialect labels for the validation set.\n",
    "\n",
    "- **y_test_names**:  \n",
    "  List of names for the testing set.\n",
    "\n",
    "- **y_test_speaker**:  \n",
    "  List of speakers for the testing set.\n",
    "\n",
    "- **y_test_segment_begin**:  \n",
    "  List of starting sample indices for segments in the testing set.\n",
    "\n",
    "- **y_test_segment_end**:  \n",
    "  List of ending sample indices for segments in the testing set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "481884a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(speaker_test_val, df, df_aug, name_aug):\n",
    "    \n",
    "    train_speaker = []\n",
    "    test_speaker = []\n",
    "    val_speaker = []\n",
    "    \n",
    "    # get data seperated by train, val and test\n",
    "    for i in range(0, len(speaker_test_val)):\n",
    "        num_val_test = len(speaker_test_val[i])//2\n",
    "        test_speaker = np.concatenate((test_speaker, speaker_test_val[i][0:num_val_test]), axis=None)\n",
    "        val_speaker = np.concatenate((val_speaker, speaker_test_val[i][num_val_test:num_val_test*2]), axis=None)\n",
    "    test = df[df['speaker'].isin(test_speaker)]\n",
    "    val = df[df['speaker'].isin(val_speaker)]\n",
    "    train_speaker = np.concatenate((test_speaker, val_speaker))\n",
    "    train = df[~df['speaker'].isin(train_speaker)]\n",
    "    \n",
    "    # choose augmented speaker Audios for train with speaker from speaker only already in train\n",
    "    if (df_aug is not None):\n",
    "        if ('Speaker_random' in name_aug):\n",
    "            elements = train['speaker'].unique().tolist()\n",
    "            dialects = train['dialect'].unique().tolist()\n",
    "            train_aug = df_aug[df_aug['speaker'].apply(lambda x: set(x).issubset(set(elements)))]\n",
    "            to_augment_number = int(len(df.index)/100*rate/len(dialects))\n",
    "\n",
    "            train_aug_res = []\n",
    "            for dialect in dialects:\n",
    "                train_aug_tmp = train_aug[train_aug['dialect'] == dialect].sample(n=to_augment_number)\n",
    "                train_aug_res = pd.concat([train_aug_res, train_aug_tmp], ignore_index=True)\n",
    "            train_aug = train_aug_res\n",
    "            train = pd.concat([train, train_aug], ignore_index=True)\n",
    "        elif (name_aug == ''):\n",
    "            train_aug = df_aug[~df_aug['speaker'].isin(train_speaker)]\n",
    "            train = train_aug\n",
    "        else:\n",
    "            train_aug = df_aug[~df_aug['speaker'].isin(train_speaker)]\n",
    "            train = pd.concat([train, train_aug], ignore_index=True)\n",
    "    \n",
    "    x_train = np.asarray(train['trillsson'].tolist())\n",
    "    y_train = train.dialect.tolist()\n",
    "    x_val = np.asarray(val['trillsson'].tolist())\n",
    "    y_val = val.dialect.tolist()\n",
    "    x_test = np.asarray(test['trillsson'].tolist())\n",
    "    y_test = test.dialect.tolist()\n",
    "    y_test_names = test.file_name.tolist()\n",
    "    y_test_speaker = test.speaker.tolist()\n",
    "    y_test_segment_begin = test.samples_begin.tolist()\n",
    "    y_test_segment_end = test.samples_end.tolist()\n",
    "   \n",
    "    return x_train, y_train, x_test, y_test, x_val, y_val, y_test_names, y_test_speaker, y_test_segment_begin, y_test_segment_end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001fb6a7",
   "metadata": {},
   "source": [
    "## get Features\n",
    "### Parameters:\n",
    "\n",
    "- **x_train**:  \n",
    "  Numpy array of features for the training set.\n",
    "\n",
    "- **y_train**:  \n",
    "  List of dialect labels for the training set.\n",
    "\n",
    "- **x_test**:  \n",
    "  Numpy array of features for the testing set.\n",
    "\n",
    "- **y_test**:  \n",
    "  List of dialect labels for the testing set.\n",
    "\n",
    "- **x_val**:  \n",
    "  Numpy array of features for the validation set.\n",
    "\n",
    "- **y_val**:  \n",
    "  List of dialect labels for the validation set.\n",
    "\n",
    "- **y_test_names**:  \n",
    "  List of names for the testing set.\n",
    "\n",
    "- **y_test_speaker**:  \n",
    "  List of speakers for the testing set.\n",
    "\n",
    "- **y_test_segment_begin**:  \n",
    "  List of starting sample indices for segments in the testing set.\n",
    "\n",
    "- **y_test_segment_end**:  \n",
    "  List of ending sample indices for segments in the testing set.\n",
    "\n",
    "- **df_learn**:  \n",
    "  DataFrame containing audio data used for encoding labels.\n",
    "\n",
    "### Returns:\n",
    "\n",
    "- **y_train**:  \n",
    "  List of shuffled dialect labels for the training set.\n",
    "\n",
    "- **x_train**:  \n",
    "  Numpy array of shuffled features for the training set.\n",
    "\n",
    "- **y_val**:  \n",
    "  List of shuffled dialect labels for the validation set.\n",
    "\n",
    "- **x_val**:  \n",
    "  Numpy array of shuffled features for the validation set.\n",
    "\n",
    "- **y_test**:  \n",
    "  List of shuffled dialect labels for the testing set.\n",
    "\n",
    "- **x_test**:  \n",
    "  Numpy array of shuffled features for the testing set.\n",
    "\n",
    "- **yy_train**:  \n",
    "  Categorical labels for the training set.\n",
    "\n",
    "- **yy_test**:  \n",
    "  Categorical labels for the testing set.\n",
    "\n",
    "- **yy_val**:  \n",
    "  Categorical labels for the validation set.\n",
    "\n",
    "- **y_test_names**:  \n",
    "  List of names for the testing set.\n",
    "\n",
    "- **y_test_speaker**:  \n",
    "  List of speakers for the testing set.\n",
    "\n",
    "- **y_test_segment_begin**:  \n",
    "  List of starting sample indices for segments in the testing set.\n",
    "\n",
    "- **y_test_segment_end**:  \n",
    "  List of ending sample indices for segments in the testing set.\n",
    "\n",
    "- **label_mapping**:  \n",
    "  Mapping of original dialect labels to encoded categorical labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5980d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(x_train, y_train, x_test, y_test, x_val, y_val, y_test_names, y_test_speaker, y_test_segment_begin, y_test_segment_end, df_learn):\n",
    "    y_train, x_train = shuffle(y_train, x_train)\n",
    "    y_val, x_val = shuffle(y_val, x_val)\n",
    "    y_test, x_test, y_test_names, y_test_speaker, y_test_segment_begin, y_test_segment_end = shuffle(y_test, x_test, y_test_names, y_test_speaker, y_test_segment_begin, y_test_segment_end)\n",
    "\n",
    "    # Encode the classification labels\n",
    "    le = LabelEncoder()\n",
    "    le.fit(sorted(df_learn['dialect'].unique().tolist()))\n",
    "    yy_train = to_categorical(le.transform(y_train))    \n",
    "    yy_test = to_categorical(le.transform(y_test))\n",
    "    yy_val = to_categorical(le.transform(y_val))\n",
    "    \n",
    "    label_mapping = dict(zip(y_train, yy_train))\n",
    "    \n",
    "    return y_train, x_train, y_val, x_val, y_test, x_test, yy_train, yy_test, yy_val, y_test_names, y_test_speaker, y_test_segment_begin, y_test_segment_end, label_mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81674b6d",
   "metadata": {},
   "source": [
    "## create model\n",
    "### Parameters:\n",
    "\n",
    "- **df_learn**:  \n",
    "  DataFrame containing audio data used for model training.\n",
    "\n",
    "- **x_train**:  \n",
    "  Numpy array of features for the training set.\n",
    "\n",
    "- **lr**:  \n",
    "  Learning rate for model optimization.\n",
    "\n",
    "- **dr**:  \n",
    "  Dropout rate for regularization.\n",
    "\n",
    "- **units**:  \n",
    "  Number of units/neurons in the dense layers of the model.\n",
    "\n",
    "- **l1_val**:  \n",
    "  L1 regularization parameter for the dense layers.\n",
    "\n",
    "- **l2_val**:  \n",
    "  L2 regularization parameter for the dense layers.\n",
    "\n",
    "- **dummy**:  \n",
    "  Boolean indicating whether to use a DummyClassifier or a custom neural network model.\n",
    "\n",
    "### Returns:\n",
    "\n",
    "- **model**:  \n",
    "  Compiled Keras model for classification.\n",
    "\n",
    "- **callback**:  \n",
    "  EarlyStopping callback to monitor validation loss and restore the best weights during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "097e57f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e72ed98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(df_learn, x_train, lr, dr, units, l1_val, l2_val, dummy):\n",
    "    \n",
    "    if dummy:\n",
    "        model = DummyClassifier(strategy='prior') #stratified #prior #uniform\n",
    "    else:\n",
    "        dialects = df_learn['dialect'].unique().tolist()\n",
    "        num_labels = len(dialects)\n",
    "        \n",
    "        f1_w = tfa.metrics.F1Score(num_classes=num_labels, average='weighted', name='f1_w')\n",
    "        #f1_w = tf.keras.metrics.F1Score(average='weighted', name='f1_w')\n",
    "\n",
    "        METRICS = [\n",
    "            keras.metrics.TruePositives(name='tp'),\n",
    "            keras.metrics.FalsePositives(name='fp'),\n",
    "            keras.metrics.TrueNegatives(name='tn'),\n",
    "            keras.metrics.FalseNegatives(name='fn'), \n",
    "            keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "            keras.metrics.Precision(name='precision'),\n",
    "            keras.metrics.Recall(name='recall'),\n",
    "            keras.metrics.AUC(name='auc'),\n",
    "            keras.metrics.AUC(name='prc', curve='PR'),\n",
    "        ]\n",
    "\n",
    "        def build_model_graph(metrics=METRICS):\n",
    "            model = Sequential()\n",
    "            model.add(Dense(units*2, input_shape=(np.array(x_train).shape[-1],),\n",
    "                            kernel_regularizer=l2(l2_val), activity_regularizer=l1(l1_val), name='Dense1'))\n",
    "            model.add(LeakyReLU(alpha=0.01))\n",
    "            model.add(Dropout(dr))\n",
    "            \n",
    "            model.add(Dense(units,\n",
    "                            kernel_regularizer=l2(l2_val), activity_regularizer=l1(l1_val), name='Dense2'))\n",
    "            model.add(LeakyReLU(alpha=0.01))  \n",
    "            model.add(Dropout(dr))\n",
    "            \n",
    "            model.add(Dense(num_labels,\n",
    "                            kernel_regularizer=l2(l2_val), activity_regularizer=l1(l1_val), name='Output'))\n",
    "            model.add(Activation('softmax'))\n",
    "\n",
    "            model.compile(loss='categorical_crossentropy', metrics=['accuracy', f1_m, f1_w], optimizer=Adam(learning_rate = lr))\n",
    "\n",
    "            return model      \n",
    "\n",
    "        model = build_model_graph()\n",
    "\n",
    "    callback = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        mode='min',\n",
    "        min_delta=0.005,\n",
    "        patience=10,\n",
    "        verbose=1,\n",
    "        baseline=None,\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "\n",
    "    return model, callback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62041397",
   "metadata": {},
   "source": [
    "## train Model\n",
    "### Parameters:\n",
    "\n",
    "- **x_train**:  \n",
    "  Numpy array of features for the training set.\n",
    "\n",
    "- **x_val**:  \n",
    "  Numpy array of features for the validation set.\n",
    "\n",
    "- **yy_train**:  \n",
    "  Categorical labels for the training set.\n",
    "\n",
    "- **yy_val**:  \n",
    "  Categorical labels for the validation set.\n",
    "\n",
    "- **class_weights**:  \n",
    "  Dictionary of class weights for handling class imbalance.\n",
    "\n",
    "- **model**:  \n",
    "  Compiled Keras model for training.\n",
    "\n",
    "- **batch_size**:  \n",
    "  Batch size used for training.\n",
    "\n",
    "- **num_epochs**:  \n",
    "  Number of epochs for training.\n",
    "\n",
    "- **callback**:  \n",
    "  EarlyStopping callback for monitoring validation loss.\n",
    "\n",
    "- **i**:  \n",
    "  Index used for TensorBoard log directory.\n",
    "\n",
    "- **lr**:  \n",
    "  Learning rate used in the model.\n",
    "\n",
    "- **dr**:  \n",
    "  Dropout rate used in the model.\n",
    "\n",
    "- **units**:  \n",
    "  Number of units/neurons in the dense layers of the model.\n",
    "\n",
    "- **l1_val**:  \n",
    "  L1 regularization parameter used in the model.\n",
    "\n",
    "- **l2_val**:  \n",
    "  L2 regularization parameter used in the model.\n",
    "\n",
    "- **tb**:  \n",
    "  Boolean indicating whether to enable TensorBoard logging.\n",
    "\n",
    "- **log_dir**:  \n",
    "  Directory path for TensorBoard logs.\n",
    "\n",
    "- **dummy**:  \n",
    "  Boolean indicating whether to train a DummyClassifier.\n",
    "\n",
    "### Returns:\n",
    "\n",
    "- **history**:  \n",
    "  History object containing training metrics and loss values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ad8750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights(y_train):\n",
    "    class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = dict(enumerate(class_weights))\n",
    "    print('Class weights:', class_weights)\n",
    "    return class_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "603170a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x_train, x_val, yy_train, yy_val, class_weights, model, batch_size, num_epochs, callback, i, lr, dr, units, l1_val, l2_val, tb, log_dir, dummy):\n",
    "      \n",
    "    if tb:\n",
    "        log_dir = log_dir + str(i) + \"lr_\" + str(lr) + \"dr_\" + str(dr) + \"units_\" + str(units) + \"l1_\" + str(l1_val) + \"l2_\" + str(l2_val) + \"bs_\" + str(batch_size)\n",
    "        tensorboard_callback = keras.callbacks.TensorBoard(\n",
    "            log_dir=(log_dir), histogram_freq=1,\n",
    "        )\n",
    "        callbacks = [callback, tensorboard_callback]\n",
    "    else:\n",
    "        callbacks = [callback]\n",
    "    \n",
    "    if dummy:\n",
    "        integer_labels = np.argmax(yy_train, axis=1)\n",
    "        history = model.fit(np.array(x_train), integer_labels)\n",
    "    else:\n",
    "        history = model.fit(np.array(x_train), yy_train, batch_size=batch_size, epochs=num_epochs,\n",
    "                            validation_data=(np.array(x_val), yy_val), verbose=1,\n",
    "                            shuffle=True, class_weight=class_weights, callbacks=callbacks)\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82518459",
   "metadata": {},
   "source": [
    "## test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4f1a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(x_test, yy_test, model, dummy):\n",
    "    pred_test = model.predict(np.array(x_test))\n",
    "\n",
    "    if dummy:\n",
    "        b = np.zeros((pred_test.size, pred_test.max() + 1))\n",
    "        b[np.arange(pred_test.size), pred_test] = 1\n",
    "        pred_test = b\n",
    "        \n",
    "    classes_x=np.argmax(pred_test,axis=1)\n",
    "    classes_true=np.argmax(yy_test,axis=1)\n",
    "    df_result = pd.DataFrame(list(zip(classes_x, classes_true)), columns=['Pred', 'True'])\n",
    "    \n",
    "    return df_result, classes_x, classes_true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b9a90e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_false(y_test_names, y_test_speaker, y_test_segment_begin, y_test_segment_end, df_result):\n",
    "    indices = df_result.index[df_result['Pred'] != df_result['True']].tolist()\n",
    "    false_names = [y_test_names[index] for index in indices]\n",
    "    false_speaker = [y_test_speaker[index] for index in indices]\n",
    "    false_segments_begin = [y_test_segment_begin[index] for index in indices]\n",
    "    false_segments_end = [y_test_segment_end[index] for index in indices]\n",
    "    false_simplified = [(y_test_names[index] + ' ' + str(y_test_segment_begin[index]/16000)) for index in indices]\n",
    "    return false_names, false_speaker, false_segments_begin, false_segments_end, false_simplified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af5f9273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def firstPictures(history, num_epochs):\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(train_loss, label='Training loss', color='#185fad')\n",
    "    plt.plot(val_loss, label='Validation loss', color='orange')\n",
    "    plt.title('Training and Validation loss by Epoch', fontsize = 25)\n",
    "    plt.xlabel('Epoch', fontsize = 18)\n",
    "    plt.xticks(range(0,num_epochs,5), range(0,num_epochs,5))\n",
    "    plt.legend(fontsize = 18)\n",
    "    plt.savefig('ex_loss_epoch_.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    train_loss = history.history['f1_m']\n",
    "    val_loss = history.history['val_f1_m']\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(train_loss, label='Training F1', color='#185fad')\n",
    "    plt.plot(val_loss, label='Validation F1', color='orange')\n",
    "    plt.title('Training and Validation F1 by Epoch', fontsize = 25)\n",
    "    plt.xlabel('Epoch', fontsize = 18)\n",
    "    plt.xticks(range(0,num_epochs,5), range(0,num_epochs,5))\n",
    "    plt.legend(fontsize = 18)\n",
    "    plt.savefig('ex_f1_epoch.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    train_loss = history.history['f1_w']\n",
    "    val_loss = history.history['val_f1_w']\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(train_loss, label='Training weighted F1', color='#185fad')\n",
    "    plt.plot(val_loss, label='Validation weighted F1', color='orange')\n",
    "    plt.title('Training and Validation weighted F1 by Epoch', fontsize = 25)\n",
    "    plt.xlabel('Epoch', fontsize = 18)\n",
    "    plt.xticks(range(0,num_epochs,5), range(0,num_epochs,5))\n",
    "    plt.legend(fontsize = 18)\n",
    "    plt.savefig('ex_weighted_f1_epoch.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    train_loss = history.history['accuracy']\n",
    "    val_loss = history.history['val_accuracy']\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(train_loss, label='Training Accuracy', color='#185fad')\n",
    "    plt.plot(val_loss, label='Validation Accuracy', color='orange')\n",
    "    plt.title('Training and Validation Accuracy by Epoch', fontsize = 25)\n",
    "    plt.xlabel('Epoch', fontsize = 18)\n",
    "    plt.xticks(range(0,num_epochs,5), range(0,num_epochs,5))\n",
    "    plt.legend(fontsize = 18)\n",
    "    plt.savefig('ex_acc_epoch.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292c6a2d",
   "metadata": {},
   "source": [
    "## main function\n",
    "### Parameters:\n",
    "\n",
    "- **first_pictures**:  \n",
    "  Boolean indicating whether to generate plots for the runs during training.\n",
    "\n",
    "- **df_learn**:  \n",
    "  DataFrame containing audio data used for model training.\n",
    "\n",
    "- **df_learn_aug**:  \n",
    "  Augmented DataFrame containing audio data, if available.\n",
    "\n",
    "- **speaker_test_val**:  \n",
    "  A list of dialects with each sublist containing a list of speakers. The first half of each sublist is designated for testing, and the second half is for validation.\n",
    "\n",
    "- **name_aug**:  \n",
    "  A name identifier used for loading augmented data or specifying augmentation settings.\n",
    "\n",
    "- **rate**:  \n",
    "  The augmentation rate, indicating the percentage of data to augment.\n",
    "\n",
    "- **i**:  \n",
    "  Index used for TensorBoard log directory.\n",
    "\n",
    "- **lr**:  \n",
    "  Learning rate used in the model.\n",
    "\n",
    "- **dr**:  \n",
    "  Dropout rate used in the model.\n",
    "\n",
    "- **units**:  \n",
    "  Number of units/neurons in the dense layers of the model.\n",
    "\n",
    "- **l1_val**:  \n",
    "  L1 regularization parameter used in the dense layers.\n",
    "\n",
    "- **l2_val**:  \n",
    "  L2 regularization parameter used in the dense layers.\n",
    "\n",
    "- **batch_size**:  \n",
    "  Batch size used for training.\n",
    "\n",
    "- **tb**:  \n",
    "  Boolean indicating whether to enable TensorBoard logging.\n",
    "\n",
    "- **log_dir**:  \n",
    "  Directory path for TensorBoard logs.\n",
    "\n",
    "- **dummy**:  \n",
    "  Boolean indicating whether to train a DummyClassifier.\n",
    "\n",
    "- **max_epochs**:  \n",
    "  Maximum number of epochs for training.\n",
    "\n",
    "### Returns:\n",
    "\n",
    "- **list_row**:  \n",
    "  A list containing various metrics and data for evaluation and analysis.\n",
    "\n",
    "- **label_mapping**:  \n",
    "  Mapping of original dialect labels to encoded categorical labels.\n",
    "\n",
    "### Description:\n",
    "\n",
    "This function orchestrates the entire workflow for training and evaluating the classification model for dialect classification. It performs data preprocessing, model creation, training, evaluation, and result extraction. Depending on the parameters, it either trains a custom neural network model or a DummyClassifier. It returns a list of evaluation metrics and data for analysis, along with a mapping of original dialect labels to encoded categorical labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eddc8869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_all(first_pictures, df_learn, df_learn_aug, speaker_test_val, name_aug, i, lr, dr, units, l1_val, l2_val, batch_size, tb, log_dir, dummy, max_epochs):\n",
    "\n",
    "    x_train, y_train, x_test, y_test, x_val, y_val, y_test_names, y_test_speaker, y_test_segment_begin, y_test_segment_end = train_test(speaker_test_val, df_learn, df_learn_aug, name_aug)\n",
    "    y_train, x_train, y_val, x_val, y_test, x_test, yy_train, yy_test, yy_val, y_test_names, y_test_speaker, y_test_segment_begin, y_test_segment_end, label_mapping = features(x_train, y_train, x_test, y_test, x_val, y_val, y_test_names, y_test_speaker, y_test_segment_begin, y_test_segment_end, df_learn)\n",
    "    class_weights = weights(y_train)\n",
    "    model, callback = create_model(df_learn, x_train, lr, dr, units, l1_val, l2_val, dummy)\n",
    "    history = train_model(x_train, x_val, yy_train, yy_val, class_weights, model, batch_size, max_epochs, callback, i, lr, dr, units, l1_val, l2_val, tb, log_dir, dummy)\n",
    "    df_result, classes_x, classes_true = pred(x_test, yy_test, model, dummy)\n",
    "    if dummy:\n",
    "        accuracy = accuracy_score(classes_true, classes_x)\n",
    "        f1 = f1_score(classes_true, classes_x, average='macro')\n",
    "        f1_w = f1_score(classes_true, classes_x, average='weighted')\n",
    "        loss = 0\n",
    "    else:\n",
    "        loss, accuracy, f1, f1_w = model.evaluate(np.array(x_test), yy_test, verbose=0)  \n",
    "    false_names, false_speaker, false_segments_begin, false_segments_end, false_simplified = get_false(y_test_names, y_test_speaker, y_test_segment_begin, y_test_segment_end, df_result)\n",
    "    if first_pictures and not dummy:\n",
    "        firstPictures(history, max_epochs)\n",
    "    \n",
    "    speaker_val = []\n",
    "    speaker_test = []\n",
    "    speaker_val = [sublist[len(sublist)//2:] for sublist in speaker_test_val]\n",
    "    speaker_test = [sublist[0:len(sublist)//2] for sublist in speaker_test_val]\n",
    "    \n",
    "    if dummy:\n",
    "        list_row = [None, None, accuracy,\n",
    "                    None, None, loss,\n",
    "                    None, None, f1,\n",
    "                    None, None, f1_w,\n",
    "                    speaker_val, speaker_test, false_names, false_speaker, false_segments_begin, false_segments_end,\n",
    "                    false_simplified, classes_x, classes_true, y_test_names, y_test_speaker]\n",
    "    else:\n",
    "        list_row = [history.history['accuracy'][-1], history.history['val_accuracy'][-1], accuracy,\n",
    "                    history.history['loss'][-1], history.history['val_loss'][-1], loss,\n",
    "                    history.history['f1_m'][-1], history.history['val_f1_m'][-1], f1,\n",
    "                    history.history['f1_w'][-1], history.history['val_f1_w'][-1], f1_w,\n",
    "                    speaker_val, speaker_test, false_names, false_speaker, false_segments_begin, false_segments_end,\n",
    "                    false_simplified, classes_x, classes_true, y_test_names, y_test_speaker]\n",
    "    \n",
    "    return list_row, label_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b503a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
